#Main

import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
import functions_eda as feda
import dictionaries as dicts
from datetime import datetime
import io
from reportlab.lib.pagesizes import letter
from reportlab.pdfgen import canvas
from reportlab.lib import colors

# Configuraci√≥n de la p√°gina
st.set_page_config(page_title="Data Quality & EDA Dashboard", layout="wide")

# --- ESTADO DE LA SESI√ìN ---
if 'logs' not in st.session_state:
    st.session_state.logs = []

if 'fb_clean_adjusted' not in st.session_state:
    st.session_state.fb_clean_adjusted = None

if 'age_outliers_log' not in st.session_state:
    st.session_state.age_outliers_log = pd.DataFrame()

if 'user_comments' not in st.session_state:
    st.session_state.user_comments = ""

def add_log(action):
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    st.session_state.logs.append({"timestamp": timestamp, "action": action})

# --- FUNCIONES DE APOYO ---
def generate_pdf_report(user_comments=""):
    buffer = io.BytesIO()
    p = canvas.Canvas(buffer, pagesize=letter)
    width, height = letter

    p.setFont("Helvetica-Bold", 16)
    p.drawString(100, height - 50, "Reporte de Calidad y Limpieza de Datos")

    p.setFont("Helvetica-Bold", 14)
    p.drawString(100, height - 80, "Historial de Acciones:")

    p.setFont("Helvetica", 10)
    y = height - 100
    for log in st.session_state.logs:
        p.drawString(100, y, f"{log['timestamp']}: {log['action']}")
        y -= 15
        if y < 100:
            p.showPage()
            y = height - 50

    if user_comments:
        y -= 20
        if y < 150:
            p.showPage()
            y = height - 50
        p.setFont("Helvetica-Bold", 14)
        p.drawString(100, y, "Comentarios del Analista:")
        y -= 20
        p.setFont("Helvetica", 10)
        # Manejo simple de multil√≠nea
        textobject = p.beginText(100, y)
        for line in user_comments.split('\n'):
            textobject.textLine(line)
        p.drawText(textobject)

    p.save()
    buffer.seek(0)
    return buffer

def compute_health_score(df_raw, df_clean):
    if df_raw is None or df_raw.empty:
        return 0, 0, 0

    total_rows = len(df_raw)
    total_nulls = df_raw.isnull().sum().sum()
    rows_cleaned = total_rows - len(df_clean)

    # Simple health score formula: (1 - (nulls / total_elements)) * 100
    total_elements = df_raw.size
    null_ratio = total_nulls / total_elements if total_elements > 0 else 0
    health_score = (1 - null_ratio) * 100

    return health_score, total_nulls, rows_cleaned

# --- SIDEBAR: CARGA DE DATOS ---
st.sidebar.title("Configuraci√≥n")
uploaded_inv = st.sidebar.file_uploader("Cargar Inventario (CSV)", type=["csv"])
uploaded_tx = st.sidebar.file_uploader("Cargar Transacciones (CSV)", type=["csv"])
uploaded_fb = st.sidebar.file_uploader("Cargar Feedback (CSV)", type=["csv"])

# Opciones de limpieza global
st.sidebar.subheader("Opciones de Limpieza")
exclude_outliers = st.sidebar.checkbox("Excluir Outliers", value=False)
exclude_nulls = st.sidebar.checkbox("Excluir Filas con Nulos", value=False)

if st.sidebar.button("Generar Log PDF"):
    pdf_buf = generate_pdf_report(st.session_state.user_comments)
    st.sidebar.download_button(label="Descargar Reporte PDF", data=pdf_buf, file_name="reporte_calidad.pdf", mime="application/pdf")

# --- L√ìGICA PRINCIPAL ---
if uploaded_inv and uploaded_tx and uploaded_fb:
    # Cargar datos
    inv_raw = pd.read_csv(uploaded_inv)
    tx_raw = pd.read_csv(uploaded_tx)
    fb_raw = pd.read_csv(uploaded_fb)

    # Pre-detecci√≥n de outliers para UI y l√≥gica (opcional si se quiere ver antes de excluir)
    # Por ahora la exclusi√≥n global se encarga.

    # 1. Proceso de Limpieza Est√°ndar
    inv_clean, inv_report = feda.sanitize_inventario(inv_raw)
    fb_clean = feda.limpiar_feedback_basico(fb_raw)

    # Aplicar ajustes persistentes de sesi√≥n si existen
    if st.session_state.fb_clean_adjusted is not None:
        fb_clean = st.session_state.fb_clean_adjusted

    # Logging inicial
    if 'initial_clean' not in st.session_state:
        add_log("Carga de archivos y limpieza inicial ejecutada.")
        st.session_state.initial_clean = True

    future_date_mode = st.sidebar.radio("Ventas Futuras", ["Mantener", "Corregir (A√±o -1)", "Excluir"])
    normalize_status = st.sidebar.checkbox("Normalizar Estado_Envio", value=False)

    # Aplicar limpieza est√°ndar de transacciones
    tx_clean, tx_report = feda.sanitize_transacciones(tx_raw, normalize_status=normalize_status)

    # 2. Procesos de Limpieza Opcionales (Transacciones)
    st.sidebar.subheader("Limpieza Opcional Transacciones")

    impute_knn = st.sidebar.checkbox("Imputar Costo Env√≠o (KNN)", value=False)
    if impute_knn:
        tx_clean = feda.imputar_costo_envio_knn(tx_clean)
        add_log("Imputaci√≥n KNN de Costo Env√≠o realizada.")

    exclude_neg_qty = st.sidebar.checkbox("Excluir Cantidades Negativas", value=False)
    if exclude_neg_qty:
        tx_clean = feda.excluir_ventas_cantidad_negativa(tx_clean)
        add_log("Cantidades negativas excluidas.")

    if future_date_mode == "Corregir (A√±o -1)":
        tx_clean = feda.corregir_o_excluir_ventas_futuras(tx_clean, modo="corregir")
        add_log("Fechas futuras corregidas.")
        # Actualizamos el reporte para reflejar la acci√≥n opcional
        future_mask = pd.to_datetime(tx_raw["Fecha_Venta"], format="%d/%m/%Y", errors="coerce") > pd.Timestamp(datetime.today().date())
        tx_report.loc[tx_report["Proceso"] == "Fechas futuras corregidas (a√±o -1)", "Filas_afectadas"] = int(future_mask.sum())
    elif future_date_mode == "Excluir":
        tx_clean = feda.corregir_o_excluir_ventas_futuras(tx_clean, modo="excluir")
        add_log("Fechas futuras excluidas.")
        # Podr√≠amos a√±adir una fila al reporte o reutilizar la existente
        future_mask = pd.to_datetime(tx_raw["Fecha_Venta"], format="%d/%m/%Y", errors="coerce") > pd.Timestamp(datetime.today().date())
        tx_report.loc[tx_report["Proceso"] == "Fechas futuras corregidas (a√±o -1)", "Proceso"] = "Fechas futuras excluidas"
        tx_report.loc[tx_report["Proceso"] == "Fechas futuras excluidas", "Filas_afectadas"] = int(future_mask.sum())
    elif future_date_mode == "Mantener":
        # Si se elige mantener, ponemos a 0 en el reporte lo que hizo sanitize_transacciones por defecto
        tx_report.loc[tx_report["Proceso"] == "Fechas futuras corregidas (a√±o -1)", "Filas_afectadas"] = 0

    include_ghost_skus = st.sidebar.checkbox("Incluir SKUs inexistentes en Inventario", value=True)
    tx_clean = feda.filtrar_skus_fantasma(tx_clean, inv_clean, incluir_fantasma=include_ghost_skus)
    if not include_ghost_skus:
        add_log("SKUs fantasma excluidos.")

    # Feedback
    exclude_fb_dupes = st.sidebar.checkbox("Excluir Feedback Duplicado", value=False)
    if exclude_fb_dupes:
        fb_clean = feda.excluir_feedback_duplicado(fb_clean)
        add_log("Feedback duplicado excluido.")

    # Exclusi√≥n global de Outliers/Nulos
    inv_clean, tx_clean, fb_clean = feda.aplicar_exclusion_global(
        inv_clean, tx_clean, fb_clean,
        exclude_outliers=exclude_outliers,
        exclude_nulls=exclude_nulls
    )
    if exclude_outliers:
        add_log("Outliers excluidos globalmente.")
    if exclude_nulls:
        add_log("Filas con nulos excluidas globalmente.")

    # JOIN Final para EDA
    joined_df = feda.build_join_dataset(tx_clean, inv_clean, fb_clean)
    joined_df = feda.feature_engineering(joined_df)

    # --- TABS ---
    tab1, tab2, tab3, tab4, tab5 = st.tabs(["EDA General", "Salud Inventario", "Salud Transacciones", "Salud NPS", "Reporte (Dashboard)"])

    with tab1:
        st.header("An√°lisis Exploratorio de Datos (EDA) por Dataset")

        # --- EDA INVENTARIO ---
        st.subheader("1. EDA: Inventario")
        col_inv1, col_inv2 = st.columns(2)
        with col_inv1:
            st.write("**Estad√≠sticas Cuantitativas**")
            st.write(inv_clean.describe())
        with col_inv2:
            st.write("**Estad√≠sticas Cualitativas**")
            st.write(inv_clean.select_dtypes(include=['object', 'string']).describe())

        st.write("**Visualizaciones Inventario**")
        num_cols_inv = inv_clean.select_dtypes(include=[np.number]).columns
        if not num_cols_inv.empty:
            st.write("*Boxplots Variables Num√©ricas*")
            # Crear un grid de columnas para los boxplots (por ejemplo 3 columnas)
            cols_grid = st.columns(3)
            for i, col in enumerate(num_cols_inv):
                with cols_grid[i % 3]:
                    fig, ax = plt.subplots(figsize=(5, 3))
                    sns.boxplot(x=inv_clean[col], ax=ax, color='skyblue')
                    ax.set_title(f"Distribuci√≥n de {col}")
                    st.pyplot(fig)

        cat_cols_inv = inv_clean.select_dtypes(include=['object', 'string']).columns
        if 'Categoria' in cat_cols_inv:
            st.write("*Distribuci√≥n por Categor√≠a*")
            fig, ax = plt.subplots(figsize=(8, 4))
            sns.countplot(y=inv_clean['Categoria'], ax=ax, palette='viridis')
            st.pyplot(fig)

        st.divider()

        # --- EDA TRANSACCIONES ---
        st.subheader("2. EDA: Transacciones")
        col_tx1, col_tx2 = st.columns(2)
        with col_tx1:
            st.write("**Estad√≠sticas Cuantitativas**")
            st.write(tx_clean.describe())
        with col_tx2:
            st.write("**Estad√≠sticas Cualitativas**")
            st.write(tx_clean.select_dtypes(include=['object', 'string']).describe())

        st.write("**Visualizaciones Transacciones**")
        num_cols_tx = tx_clean.select_dtypes(include=[np.number]).columns
        if not num_cols_tx.empty:
            st.write("*Boxplots Variables Num√©ricas*")
            cols_grid_tx = st.columns(3)
            for i, col in enumerate(num_cols_tx):
                with cols_grid_tx[i % 3]:
                    fig, ax = plt.subplots(figsize=(5, 3))
                    sns.boxplot(x=tx_clean[col], ax=ax, color='lightgreen')
                    ax.set_title(f"Distribuci√≥n de {col}")
                    st.pyplot(fig)

        if 'Canal_Venta' in tx_clean.columns:
            st.write("*Distribuci√≥n por Canal de Venta*")
            fig, ax = plt.subplots(figsize=(8, 4))
            sns.countplot(x=tx_clean['Canal_Venta'], ax=ax, palette='magma')
            st.pyplot(fig)

        st.divider()

        # --- EDA FEEDBACK ---
        st.subheader("3. EDA: Feedback")
        col_fb1, col_fb2 = st.columns(2)
        with col_fb1:
            st.write("**Estad√≠sticas Cuantitativas**")
            st.write(fb_clean.describe())
        with col_fb2:
            st.write("**Estad√≠sticas Cualitativas**")
            st.write(fb_clean.select_dtypes(include=['object', 'string']).describe())

        st.write("**Visualizaciones Feedback**")
        num_cols_fb = fb_clean.select_dtypes(include=[np.number]).columns
        if not num_cols_fb.empty:
            st.write("*Boxplots Variables Num√©ricas*")
            cols_grid_fb = st.columns(3)
            for i, col in enumerate(num_cols_fb):
                with cols_grid_fb[i % 3]:
                    fig, ax = plt.subplots(figsize=(5, 3))
                    sns.boxplot(x=fb_clean[col], ax=ax, color='salmon')
                    ax.set_title(f"Distribuci√≥n de {col}")
                    st.pyplot(fig)

        if 'Satisfaccion_NPS_Grupo' in fb_clean.columns:
            st.write("*Distribuci√≥n Grupos NPS*")
            fig, ax = plt.subplots(figsize=(8, 4))
            sns.countplot(x=fb_clean['Satisfaccion_NPS_Grupo'], ax=ax, palette='rocket')
            plt.xticks(rotation=45)
            st.pyplot(fig)

    with tab2:
        st.header("An√°lisis de Salud: Inventario")
        h_score, nulls, cleaned = compute_health_score(inv_raw, inv_clean)

        c1, c2, c3 = st.columns(3)
        c1.metric("Health Score", f"{h_score:.2f}%")
        c2.metric("Nulos Totales (Raw)", nulls)
        c3.metric("Filas Removidas/Filtradas", cleaned)

        st.subheader("Reporte de Procesos de Limpieza")
        st.table(inv_report)

        st.subheader("Muestra de Datos Limpios")
        st.dataframe(inv_clean.head(20))

    with tab3:
        st.header("An√°lisis de Salud: Transacciones")

        # Gr√°fico de cantidades negativas
        neg_qty_df = tx_raw[tx_raw['Cantidad_Vendida'] < 0]
        if not neg_qty_df.empty:
            st.subheader("An√°lisis de Cantidades Negativas")
            col_v1, col_v2 = st.columns(2)
            with col_v1:
                st.write(f"Total de registros con cantidad negativa: {len(neg_qty_df)}")
                st.dataframe(neg_qty_df[['Transaccion_ID', 'SKU_ID', 'Cantidad_Vendida']].head(10))
            with col_v2:
                fig, ax = plt.subplots(figsize=(5, 3))
                sns.histplot(neg_qty_df['Cantidad_Vendida'], ax=ax, bins=20, color='red')
                ax.set_title("Distribuci√≥n de Cantidades Negativas")
                st.pyplot(fig)
        else:
            st.success("No se encontraron cantidades negativas en los datos originales.")

        h_score, nulls, cleaned = compute_health_score(tx_raw, tx_clean)

        c1, c2, c3 = st.columns(3)
        c1.metric("Health Score", f"{h_score:.2f}%")
        c2.metric("Nulos Totales (Raw)", nulls)
        c3.metric("Filas Removidas/Filtradas", cleaned)

        # ghost SKU metrics
        st.subheader("Productos en Transacciones no existentes en Inventario")
        ghost_skus = tx_clean[tx_clean['flag_sku_fantasma'] == True]

        total_sales = (tx_clean['Cantidad_Vendida'] * tx_clean['Precio_Venta_Final']).sum()
        ghost_sales = (ghost_skus['Cantidad_Vendida'] * ghost_skus['Precio_Venta_Final']).sum()
        ghost_pct = (ghost_sales / total_sales * 100) if total_sales > 0 else 0

        gc1, gc2, gc3, gc4 = st.columns(4)
        gc1.metric("SKUs √önicos Faltantes", ghost_skus['SKU_ID'].nunique())
        gc2.metric("Transacciones Afectadas", len(ghost_skus))
        gc3.metric("Ventas Fantasma (USD)", f"${ghost_sales:,.2f}")
        gc4.metric("% del Total de Ventas", f"{ghost_pct:.2f}%",
                   delta=None,
                   delta_color="inverse")

        # Interpretaci√≥n contextual
        if ghost_pct > 10:
            st.error(f"‚ö†Ô∏è Las ventas fantasma representan m√°s del 10% del total. **Acci√≥n cr√≠tica requerida.**")
        elif ghost_pct > 5:
            st.warning(f"‚ö†Ô∏è Las ventas fantasma superan el 5%. Se recomienda revisi√≥n urgente del cat√°logo.")
        else:
            st.info(f"‚úÖ Las ventas fantasma est√°n bajo control ({ghost_pct:.2f}%).")

        if not ghost_skus.empty:
            st.dataframe(ghost_skus[['SKU_ID', 'Transaccion_ID', 'Cantidad_Vendida', 'Precio_Venta_Final']].head(10))

        st.subheader("Reporte de Procesos de Limpieza")
        st.table(tx_report)

    with tab4:
        st.header("An√°lisis de Salud: NPS (Feedback)")
        h_score, nulls, cleaned = compute_health_score(fb_raw, fb_clean)

        c1, c2, c3 = st.columns(3)
        c1.metric("Health Score", f"{h_score:.2f}%")
        c2.metric("Nulos Totales (Raw)", nulls)
        c3.metric("Filas Removidas/Filtradas", cleaned)

        if 'Satisfaccion_NPS' in fb_clean.columns:
            # --- NUEVA VISUALIZACI√ìN NPS PROFESIONAL ---
            st.markdown("---")

            # 1. Preparaci√≥n de Datos
            nps_scores = fb_clean['Satisfaccion_NPS'].value_counts().reindex(range(11), fill_value=0)
            total_respuestas = len(fb_clean)

            detractores = fb_clean[fb_clean['Satisfaccion_NPS'] <= 0]
            pasivos = fb_clean[(fb_clean['Satisfaccion_NPS'] >= 0.1) & (fb_clean['Satisfaccion_NPS'] <= 50)]
            promotores = fb_clean[fb_clean['Satisfaccion_NPS'] >= 50.1]

            count_det = len(detractores)
            count_pas = len(pasivos)
            count_pro = len(promotores)

            pct_det = (count_det / total_respuestas * 100) if total_respuestas > 0 else 0
            pct_pas = (count_pas / total_respuestas * 100) if total_respuestas > 0 else 0
            pct_pro = (count_pro / total_respuestas * 100) if total_respuestas > 0 else 0

            nps_score = pct_pro - pct_det

            # Colores corporativos
            color_det = "#E91E63"
            color_pas = "#FFC107"
            color_pro = "#4CAF50"

            # --- FILA 2: F√≥rmula y M√©tricas ---
            st.markdown(f"<h3 style='text-align: center;'>NPS = <span style='color:{color_pro}'>%PROMOTORES</span> - "
                        f"<span style='color:{color_det}'>%DETRACTORES</span></h3>", unsafe_allow_html=True)

            col_met1, col_met2, col_met3 = st.columns([1.5, 3, 1])

            with col_met1:
                # Donut Chart
                fig_donut, ax_donut = plt.subplots(figsize=(4, 4))
                sizes = [max(0.1, pct_pro), max(0.1, pct_pas), max(0.1, pct_det)]

                ax_donut.pie(sizes, colors=[color_pro, color_pas, color_det], startangle=90, counterclock=False,
                             wedgeprops={'width': 0.3, 'edgecolor': 'w', 'linewidth': 2})

                ax_donut.text(0, 0, f"{int(nps_score)}", fontsize=40, ha='center', va='center', fontweight='bold')
                st.pyplot(fig_donut)

            with col_met2:
                st.write("") # Espaciador
                # Estilo de las cajas de m√©tricas
                def metric_box(label, count, pct, color):
                    st.markdown(f"""
                        <div style='display: flex; align-items: center; margin-bottom: 10px;'>
                            <div style='background-color:{color}; color:white; padding:10px 20px; border-radius:20px; width:150px; text-align:center; font-weight:bold; margin-right:10px;'>{label}</div>
                            <div style='background-color:{color}; color:white; padding:10px; border-radius:50%; width:40px; height:40px; display:flex; align-items:center; justify-content:center; font-weight:bold; margin-right:10px;'>{count}</div>
                            <div style='background-color:#F0F2F6; padding:10px 20px; border-radius:20px; width:100px; text-align:center; font-weight:bold;'>{pct:.1f}%</div>
                        </div>
                    """, unsafe_allow_html=True)

                metric_box("detractores", count_det, pct_det, color_det)
                metric_box("pasivos", count_pas, pct_pas, color_pas)
                metric_box("promotores", count_pro, pct_pro, color_pro)

            with col_met3:
                st.markdown(f"""
                    <div style='text-align: center; margin-top: 50px;'>
                        <div style='background-color:#333; color:white; padding:5px 15px; border-radius:15px; display:inline-block; font-weight:bold;'>total</div>
                        <div style='background-color:#AAA; color:white; width:80px; height:80px; border-radius:50%; display:flex; align-items:center; justify-content:center; font-size:24px; font-weight:bold; margin: 10px auto;'>{total_respuestas}</div>
                    </div>
                """, unsafe_allow_html=True)

            st.markdown("---")
        else:
            st.warning("No se encontr√≥ informaci√≥n de NPS procesada.")

        if 'Edad_Cliente' in fb_clean.columns:
            st.subheader("An√°lisis de Outliers en Edad")

            # Funci√≥n para ajustar outliers de edad
            if st.button("Ajustar Outliers de Edad a la Mediana"):
                median_age = fb_clean[fb_clean['Edad_Cliente'] <= 100]['Edad_Cliente'].median()
                outliers_mask = fb_clean['Edad_Cliente'] > 100
                num_outliers = outliers_mask.sum()
                if num_outliers > 0:
                    # Guardar los datos outliers antes de imputar para el log
                    current_outliers = fb_clean[outliers_mask].copy()
                    st.session_state.age_outliers_log = pd.concat([st.session_state.age_outliers_log, current_outliers]).drop_duplicates()

                    fb_clean.loc[outliers_mask, 'Edad_Cliente'] = median_age
                    st.session_state.fb_clean_adjusted = fb_clean.copy()
                    add_log(f"Se ajustaron {num_outliers} outliers de edad a la mediana ({median_age}).")
                    st.success(f"Se han ajustado {num_outliers} registros.")
                    # Forzamos recarga para ver cambios en gr√°ficas
                    st.rerun()
                else:
                    st.info("No se encontraron outliers (> 100 a√±os) para ajustar.")

            # Mostrar log de cambios de edad si existe
            if not st.session_state.age_outliers_log.empty:
                with st.expander("Ver log de outliers de edad procesados"):
                    st.write("Registros que superaban los 100 a√±os y fueron ajustados:")
                    st.dataframe(st.session_state.age_outliers_log)

            age_outliers = feda.outlier_flag_iqr(fb_clean, 'Edad_Cliente')
            outlier_df = fb_clean[age_outliers]

            if not outlier_df.empty:
                st.warning(f"Se detectaron {len(outlier_df)} registros con edades fuera de rango.")
                col_age1, col_age2 = st.columns(2)
                with col_age1:
                    st.write("Muestra de Outliers:")
                    st.dataframe(outlier_df[['Feedback_ID', 'Edad_Cliente', 'Satisfaccion_NPS']].head(10))
                with col_age2:
                    fig, ax = plt.subplots(figsize=(5, 3))
                    sns.boxplot(x=fb_clean['Edad_Cliente'], ax=ax, color='orange')
                    ax.set_title("Boxplot de Edad (Feedback)")
                    st.pyplot(fig)
            else:
                st.success("No se detectaron outliers significativos en la edad de los encuestados.")
        else:
            # Si no existe la columna 'Edad', informamos (podr√≠a ser opcional en el CSV)
            st.info("La columna 'Edad' no est√° presente en el dataset de Feedback para el an√°lisis de outliers.")


        # NUEVA SECCI√ìN: An√°lisis Comparativo Tickets vs NPS
        st.divider()
        st.subheader("üìä Comparativo: Tickets Abiertos (Si vs No)")

        if 'Ticket_Soporte_Abierto_Limpio' in joined_df.columns:
            ticket_counts = joined_df['Ticket_Soporte_Abierto_Limpio'].value_counts().reset_index()
            ticket_counts.columns = ['Ticket_Abierto', 'Cantidad']

            # Gr√°fico de barras
            fig_tickets = px.bar(
                ticket_counts,
                x='Ticket_Abierto',
                y='Cantidad',
                color='Ticket_Abierto',
                text='Cantidad',
                title='Cantidad de Registros: Tickets Abiertos = Si vs No',
                labels={'Ticket_Abierto': 'Estado del Ticket', 'Cantidad': 'N√∫mero de Registros'},
                color_discrete_map={'Si': '#E91E63', 'No': '#4CAF50'}
            )
            fig_tickets.update_traces(textposition='outside')
            fig_tickets.update_layout(showlegend=False, height=450)
            st.plotly_chart(fig_tickets, use_container_width=True)

            # M√©tricas
            col_m1, col_m2, col_m3 = st.columns(3)
            cant_si = ticket_counts[ticket_counts['Ticket_Abierto'] == 'Si']['Cantidad'].values
            cant_no = ticket_counts[ticket_counts['Ticket_Abierto'] == 'No']['Cantidad'].values
            cant_si = cant_si[0] if len(cant_si) > 0 else 0
            cant_no = cant_no[0] if len(cant_no) > 0 else 0
            total = cant_si + cant_no

            col_m1.metric("Tickets Abiertos = Si", f"{cant_si:,}")
            col_m2.metric("Tickets Abiertos = No", f"{cant_no:,}")
            col_m3.metric("% con Tickets", f"{(cant_si/total*100):.1f}%" if total > 0 else "0%")

            st.dataframe(ticket_counts, use_container_width=True)
        else:
            st.info("‚ÑπÔ∏è Columna 'Ticket_Soporte_Abierto_Limpio' no encontrada.")

    with tab5:
        st.header("üìä Reporte de Calidad y An√°lisis de Negocio")

        # SECCI√ìN 1: M√©tricas de Calidad de Datos
        st.subheader("1Ô∏è‚É£ M√©tricas de Calidad de Datos")

        def get_top_nulls(df):
            nulls = df.isnull().sum()
            return nulls[nulls > 0].sort_values(ascending=False).head(5)

        def count_outliers_multicol(df):
            # Usamos la l√≥gica de feda para consistencia
            df_out = feda.detectar_outliers_multicolumna(df)
            total = len(df_out)
            count = df_out["Es_Outlier"].sum()
            pct = (count / total * 100) if total > 0 else 0
            return count, pct

        # M√©tricas Inventario
        inv_out_count, inv_out_pct = count_outliers_multicol(inv_raw)
        inv_metrics = {
            "Dataset": "Inventario",
            "Registros Totales (Raw)": len(inv_raw),
            "Registros Finales (Clean)": len(inv_clean),
            "% Nulidad General": (inv_raw.isnull().sum().sum() / inv_raw.size * 100),
            "Top 5 Nulos": str(get_top_nulls(inv_raw).to_dict()),
            "Duplicados": inv_raw.duplicated().sum(),
            "Outliers": f"{inv_out_count} ({inv_out_pct:.1f}%)"
        }

        # M√©tricas Transacciones
        tx_out_count, tx_out_pct = count_outliers_multicol(tx_raw)
        tx_metrics = {
            "Dataset": "Transacciones",
            "Registros Totales (Raw)": len(tx_raw),
            "Registros Finales (Clean)": len(tx_clean),
            "% Nulidad General": (tx_raw.isnull().sum().sum() / tx_raw.size * 100),
            "Top 5 Nulos": str(get_top_nulls(tx_raw).to_dict()),
            "Duplicados": tx_raw.duplicated().sum(),
            "Outliers": f"{tx_out_count} ({tx_out_pct:.1f}%)"
        }

        # M√©tricas Feedback
        fb_out_count, fb_out_pct = count_outliers_multicol(fb_raw)
        fb_metrics = {
            "Dataset": "Feedback",
            "Registros Totales (Raw)": len(fb_raw),
            "Registros Finales (Clean)": len(fb_clean),
            "% Nulidad General": (fb_raw.isnull().sum().sum() / fb_raw.size * 100),
            "Top 5 Nulos": str(get_top_nulls(fb_raw).to_dict()),
            "Duplicados": fb_raw.duplicated().sum(),
            "Outliers": f"{fb_out_count} ({fb_out_pct:.1f}%)"
        }

        df_quality = pd.DataFrame([inv_metrics, tx_metrics, fb_metrics])
        st.table(df_quality)

        # Visualizaci√≥n: Barras Apiladas
        st.write("**Registros Originales vs Limpios vs Excluidos**")
        data_viz = {
            'Dataset': ['Inventario', 'Transacciones', 'Feedback'],
            'Limpios': [len(inv_clean), len(tx_clean), len(fb_clean)],
            'Excluidos': [len(inv_raw)-len(inv_clean), len(tx_raw)-len(tx_clean), len(fb_raw)-len(fb_clean)]
        }
        df_viz = pd.DataFrame(data_viz)

        fig_qual, ax_qual = plt.subplots(figsize=(10, 5))
        df_viz.set_index('Dataset').plot(kind='bar', stacked=True, color=['#4CAF50', '#E91E63'], ax=ax_qual)
        ax_qual.set_title("Estado de los Registros por Dataset")
        ax_qual.set_ylabel("Cantidad de Registros")
        st.pyplot(fig_qual)

        st.divider()

        # SECCI√ìN 2: Decisiones √âticas de Limpieza
        st.subheader("2Ô∏è‚É£ Decisiones √âticas de Limpieza")
        col_log1, col_log2 = st.columns([2, 1])

        with col_log1:
            st.write("**Log de Acciones de Limpieza**")
            if st.session_state.logs:
                log_df = pd.DataFrame(st.session_state.logs)
                st.dataframe(log_df, use_container_width=True)
            else:
                st.info("No hay acciones registradas a√∫n.")

        with col_log2:
            st.write("**Comentarios del Analista**")
            st.session_state.user_comments = st.text_area("Justificaciones √©ticas y observaciones:",
                                                          value=st.session_state.user_comments,
                                                          placeholder="Escriba aqu√≠ sus comentarios...",
                                                          height=200)

        st.write("**Resumen de Decisiones de Imputaci√≥n/Limpieza**")
        imputacion_data = [
            {"Variable": "Stock_Actual", "Acci√≥n Tomada": "Conversi√≥n/Imputaci√≥n", "M√©todo": "Absoluto y Fillna(0)", "Justificaci√≥n": "Stock no puede ser negativo; nulos se asumen sin existencias."},
            {"Variable": "Costo_Envio", "Acci√≥n Tomada": "Imputaci√≥n (Opcional)", "M√©todo": "KNN Imputer", "Justificaci√≥n": "Recuperar datos perdidos basados en cercan√≠a de otras variables log√≠sticas."},
            {"Variable": "Edad_Cliente", "Acci√≥n Tomada": "Imputaci√≥n", "M√©todo": "Mediana", "Justificaci√≥n": "Valores > 100 son outliers biol√≥gicos improbables; se ajustan a la tendencia central."},
            {"Variable": "Fechas_Venta", "Acci√≥n Tomada": "Correcci√≥n/Exclusi√≥n", "M√©todo": "A√±o -1 / Drop", "Justificaci√≥n": "Datos futuros son errores de entrada; se corrigen si es posible o se eliminan para no sesgar el hist√≥rico."}
        ]
        st.table(pd.DataFrame(imputacion_data))

        # Distribuciones antes/despu√©s (Ejemplo con Edad si fue ajustada)
        if st.session_state.fb_clean_adjusted is not None:
            st.write("**Impacto de la Imputaci√≥n: Edad_Cliente**")
            fig_age, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))
            sns.histplot(fb_raw['Edad_Cliente'].dropna(), ax=ax1, color='gray', kde=True)
            ax1.set_title("Antes (Raw)")
            sns.histplot(fb_clean['Edad_Cliente'], ax=ax2, color='salmon', kde=True)
            ax2.set_title("Despu√©s (Limpios/Ajustados)")
            st.pyplot(fig_age)

        st.divider()

        # SECCI√ìN 3: An√°lisis de SKUs Fantasma
        st.subheader("3Ô∏è‚É£ An√°lisis de SKUs Fantasma")
        ghost_skus_all = tx_clean[tx_clean['flag_sku_fantasma'] == True]

        if not ghost_skus_all.empty:
            total_ghost_unique = ghost_skus_all['SKU_ID'].nunique()
            total_tx_ghost = len(ghost_skus_all)
            pct_tx_ghost = (total_tx_ghost / len(tx_clean) * 100)

            ghost_sales_val = (ghost_skus_all['Cantidad_Vendida'] * ghost_skus_all['Precio_Venta_Final']).sum()
            total_sales_val = (tx_clean['Cantidad_Vendida'] * tx_clean['Precio_Venta_Final']).sum()
            pct_sales_ghost = (ghost_sales_val / total_sales_val * 100) if total_sales_val > 0 else 0

            gs_c1, gs_c2, gs_c3 = st.columns(3)
            gs_c1.metric("SKUs √önicos Fantasma", total_ghost_unique)
            gs_c2.metric("Transacciones Afectadas", f"{total_tx_ghost} ({pct_tx_ghost:.2f}%)")
            gs_c3.metric("Ventas Totales Fantasma", f"${ghost_sales_val:,.2f} ({pct_sales_ghost:.2f}%)")

            # Gr√°fico de barras horizontal TOP 10
            st.write("**Top 10 SKUs Fantasma por Frecuencia**")
            top_ghost = ghost_skus_all['SKU_ID'].value_counts().head(10).reset_index()
            top_ghost.columns = ['SKU_ID', 'Frecuencia']

            fig_ghost, ax_ghost = plt.subplots(figsize=(8, 5))
            sns.barplot(x='Frecuencia', y='SKU_ID', data=top_ghost, palette='Oranges_r', ax=ax_ghost)
            ax_ghost.set_title("SKUs Fantasma m√°s frecuentes")
            st.pyplot(fig_ghost)

            with st.expander("Ver Tabla Detallada de SKUs Fantasma"):
                # Agrupaci√≥n para tabla detallada
                ghost_details = ghost_skus_all.groupby('SKU_ID').agg({
                    'Transaccion_ID': 'count',
                    'Cantidad_Vendida': 'sum',
                    'Precio_Venta_Final': 'sum' # Esto es suma de precios, quiz√°s mejor Ingreso
                }).reset_index()
                ghost_details.columns = ['SKU_ID', 'Transacciones', 'Cant_Total', 'Ingreso_Total']
                ghost_details = ghost_details.sort_values('Ingreso_Total', ascending=False)
                st.dataframe(ghost_details, use_container_width=True)
        else:
            st.success("No se detectaron SKUs fantasma en el dataset actual.")

        st.divider()

        # SECCI√ìN 4: Margen Negativo (Fuga de Capital)
        st.subheader("4Ô∏è‚É£ Fuga de Capital: SKUs con Margen Negativo")

        margen_negativo = joined_df[joined_df['Margen_Neto_aprox'] < 0].copy()

        if not margen_negativo.empty:
            loss_unique_skus = margen_negativo['SKU_ID'].nunique()
            loss_tx_count = len(margen_negativo)
            total_loss = margen_negativo['Margen_Neto_aprox'].sum()

            total_revenue = joined_df['Ingreso'].sum()
            pct_loss_tx = (loss_tx_count / len(joined_df) * 100) if len(joined_df) > 0 else 0

            mn_c1, mn_c2, mn_c3 = st.columns(3)
            mn_c1.metric("SKUs con Margen Negativo", loss_unique_skus)
            mn_c2.metric("Transacciones con P√©rdida", f"{loss_tx_count} ({pct_loss_tx:.2f}%)")
            mn_c3.metric("P√©rdida Total Acumulada", f"${total_loss:,.2f}", delta_color="inverse")

            # Gr√°fico Top 10 P√©rdidas
            st.write("**Top 10 SKUs con Mayor P√©rdida Acumulada**")
            perdidas_por_sku = margen_negativo.groupby('SKU_ID').agg({
                'Margen_Neto_aprox': 'sum',
                'Transaccion_ID': 'count',
                'Canal_Venta': lambda x: x.mode()[0] if not x.empty else 'Desconocido'
            }).reset_index()
            perdidas_por_sku.columns = ['SKU_ID', 'Perdida_Total', 'Num_Transacciones', 'Canal_Principal']
            top_perdidas = perdidas_por_sku.sort_values('Perdida_Total').head(10) # M√°s negativo

            fig_loss, ax_loss = plt.subplots(figsize=(8, 5))
            sns.barplot(x=top_perdidas['Perdida_Total'].abs(), y=top_perdidas['SKU_ID'], palette='Reds_r', ax=ax_loss)
            ax_loss.set_title("Top 10 SKUs con mayor Fuga de Capital")
            ax_loss.set_xlabel("P√©rdida Acumulada (Valor Absoluto)")
            st.pyplot(fig_loss)

            st.write("**Distribuci√≥n de Margen Negativo por Canal de Venta**")
            canal_loss = margen_negativo.groupby('Canal_Venta')['Margen_Neto_aprox'].sum().reset_index()
            st.table(canal_loss.sort_values('Margen_Neto_aprox'))
        else:
            st.success("No se detectaron transacciones con margen neto negativo.")

        st.divider()

        # SECCI√ìN 5: Diagn√≥stico de Fidelidad (Stock Alto vs NPS Bajo)
        st.subheader("5Ô∏è‚É£ Diagn√≥stico de Fidelidad: Paradoja Stock Alto + NPS Bajo")

        # Filtrar datos v√°lidos
        fidelidad_df = joined_df[
            (joined_df['Stock_Actual'].notna()) &
            (joined_df['Stock_Actual'] > 0) &
            (joined_df['Satisfaccion_NPS'].notna()) &
            (joined_df['Satisfaccion_NPS_Grupo'].notna())
            ].copy()

        if len(fidelidad_df) > 0:
            # Agregar por SKU
            fidelidad_agg = fidelidad_df.groupby('SKU_ID').agg({
                'Stock_Actual': 'mean',
                'Satisfaccion_NPS': 'mean',
                'Satisfaccion_NPS_Grupo': lambda x: x.mode()[0] if len(x) > 0 else 'desconocido',
                'Categoria': 'first',
                'Precio_Venta_Final': 'mean',
                'Transaccion_ID': 'count'
            }).reset_index()
            fidelidad_agg.columns = ['SKU_ID', 'Stock_Promedio', 'NPS_Promedio', 'NPS_Grupo', 'Categoria', 'Precio_Promedio', 'Num_Ventas']

            # Identificar paradoja
            stock_percentil_75 = fidelidad_agg['Stock_Promedio'].quantile(0.75)
            nps_bajo = fidelidad_agg['NPS_Promedio'] < 50

            paradoja_df = fidelidad_agg[
                (fidelidad_agg['Stock_Promedio'] >= stock_percentil_75) &
                (nps_bajo)
                ].copy()

            # Mapa de colores por grupo NPS
            color_map = {
                'muy_insatisfecho': '#E91E63',
                'neutro_o_ligeramente_satisfecho': '#FFC107',
                'satisfecho': '#8BC34A',
                'muy_satisfecho': '#4CAF50'
            }

            # Scatter plot
            fig_fidelidad = px.scatter(
                fidelidad_agg,
                x='Stock_Promedio',
                y='NPS_Promedio',
                color='NPS_Grupo',
                size='Num_Ventas',
                hover_data=['SKU_ID', 'Categoria', 'Precio_Promedio', 'Num_Ventas'],
                color_discrete_map=color_map,
                title='Relaci√≥n entre Stock Disponible y Satisfacci√≥n del Cliente (NPS)',
                labels={
                    'Stock_Promedio': 'Stock Promedio (unidades)',
                    'NPS_Promedio': 'Satisfacci√≥n NPS Promedio',
                    'NPS_Grupo': 'Grupo NPS',
                    'Num_Ventas': 'Volumen de Ventas'
                },
                size_max=20
            )

            # L√≠neas de referencia
            fig_fidelidad.add_hline(y=50, line_dash="dash", line_color="gray",
                                    annotation_text="Umbral NPS Aceptable (50)",
                                    annotation_position="right")
            fig_fidelidad.add_vline(x=stock_percentil_75, line_dash="dash", line_color="blue",
                                    annotation_text=f"P75 Stock ({stock_percentil_75:.0f})",
                                    annotation_position="top")

            # Zona problem√°tica
            fig_fidelidad.add_shape(
                type="rect",
                x0=stock_percentil_75, x1=fidelidad_agg['Stock_Promedio'].max() * 1.1,
                y0=0, y1=50,
                fillcolor="red", opacity=0.1, line_width=0
            )

            fig_fidelidad.update_layout(height=500)
            st.plotly_chart(fig_fidelidad, use_container_width=True)

            # An√°lisis de productos parad√≥jicos
            st.write("**‚ö†Ô∏è Productos Parad√≥jicos: Alto Stock + Baja Satisfacci√≥n**")

            if len(paradoja_df) > 0:
                st.write(f"Se encontraron **{len(paradoja_df)} productos** con alta disponibilidad pero baja satisfacci√≥n:")

                paradoja_display = paradoja_df[['SKU_ID', 'Categoria', 'Stock_Promedio', 'NPS_Promedio', 'Precio_Promedio', 'Num_Ventas']].sort_values('NPS_Promedio')
                st.dataframe(paradoja_display.head(10), use_container_width=True)

                # An√°lisis por categor√≠a
                st.write("**Distribuci√≥n por Categor√≠a:**")
                cat_counts = paradoja_df['Categoria'].value_counts()
                fig_cat = px.bar(
                    x=cat_counts.index,
                    y=cat_counts.values,
                    labels={'x': 'Categor√≠a', 'y': 'Cantidad de SKUs'},
                    title='Categor√≠as m√°s afectadas por la paradoja',
                    color=cat_counts.values,
                    color_continuous_scale='Reds'
                )
                st.plotly_chart(fig_cat, use_container_width=True)

                # Interpretaci√≥n
                st.markdown("""
                ### üßê Posibles Causas:
                
                1. **Problema de Calidad**: Los productos tienen defectos o no cumplen expectativas
                2. **Sobrecosto**: El precio es alto comparado con el valor percibido
                3. **Descatalogaci√≥n inminente**: Stock alto por falta de demanda (c√≠rculo vicioso)
                4. **Problema log√≠stico**: Entregas deficientes afectan la percepci√≥n del producto
                
                ### üí° Recomendaciones:
                - Revisar reviews/comentarios de estos productos espec√≠ficos
                - Comparar precios con competencia
                - Evaluar promociones o descuentos para reducir inventario
                - Considerar mejora de calidad o retiro del cat√°logo
                """)
            else:
                st.success("‚úÖ No se detectaron productos con la paradoja de alto stock y baja satisfacci√≥n.")
        else:
            st.warning("‚ö†Ô∏è No hay datos suficientes para el an√°lisis de fidelidad (requiere stock y NPS).")

        st.divider()

        # SECCI√ìN 6: Crisis Log√≠stica y Correlaci√≥n NPS
        st.subheader("6Ô∏è‚É£ Crisis Log√≠stica: Correlaci√≥n Tiempo de Entrega vs NPS")

        # Filtrar datos v√°lidos
        analisis_log = joined_df[
            (joined_df['Tiempo_Entrega_Real'].notna()) &
            (joined_df['Satisfaccion_NPS'].notna()) &
            (joined_df['Bodega_Origen'].notna()) &
            (joined_df['Ciudad_Destino'].notna())
            ].copy()

        if not analisis_log.empty:
            # Correlaci√≥n por Bodega-Ciudad
            def get_corr(x):
                if len(x) > 5:
                    return x[['Tiempo_Entrega_Real', 'Satisfaccion_NPS']].corr().iloc[0, 1]
                return np.nan

            res_corr = analisis_log.groupby(['Bodega_Origen', 'Ciudad_Destino']).apply(get_corr).reset_index(name='Correlacion')

            metricas_log = analisis_log.groupby(['Bodega_Origen', 'Ciudad_Destino']).agg({
                'Tiempo_Entrega_Real': 'mean',
                'Satisfaccion_NPS': 'mean',
                'Transaccion_ID': 'count'
            }).reset_index()

            resultado_log = res_corr.merge(metricas_log, on=['Bodega_Origen', 'Ciudad_Destino'])
            resultado_log = resultado_log.sort_values('Correlacion')

            # Heatmap
            st.write("**Heatmap de Correlaci√≥n: Tiempo Entrega vs NPS**")
            pivot_corr = resultado_log.pivot(index="Ciudad_Destino", columns="Bodega_Origen", values="Correlacion")

            fig_heat, ax_heat = plt.subplots(figsize=(10, 8))
            sns.heatmap(pivot_corr, annot=True, cmap='RdYlGn', center=0, ax=ax_heat)
            ax_heat.set_title("Correlaci√≥n de Pearson (Negativo = Rojo)")
            st.pyplot(fig_heat)

            # Identificaci√≥n zona cr√≠tica
            criticos = resultado_log[(resultado_log['Correlacion'] < -0.5) & (resultado_log['Transaccion_ID'] > 2)]
            if not criticos.empty:
                peor = criticos.iloc[0]
                st.error(f"üö® **ZONA CR√çTICA DETECTADA:** La ruta desde **{peor['Bodega_Origen']}** hacia **{peor['Ciudad_Destino']}** tiene una correlaci√≥n de **{peor['Correlacion']:.2f}**. Requiere cambio inmediato de operador log√≠stico.")

            with st.expander("Ver Tabla Completa de Correlaci√≥n Log√≠stica"):
                st.dataframe(resultado_log, use_container_width=True)
        else:
            st.info("No hay suficientes datos cruzados (Entrega + NPS) para realizar el an√°lisis de correlaci√≥n.")

        st.divider()

        # SECCI√ìN 7: Riesgo Operativo (Antig√ºedad Revisi√≥n vs Tickets)
        st.subheader("7Ô∏è‚É£ Riesgo Operativo: Antig√ºedad de Revisi√≥n vs Tickets de Soporte")

        st.markdown("""
        **Pregunta clave:** ¬øQu√© bodegas est√°n operando "a ciegas" (sin revisar su inventario) 
        y c√≥mo impacta esto en la satisfacci√≥n del cliente?
        """)

        # Verificar columnas necesarias
        columnas_necesarias = ['Ultima_Revision', 'Ticket_Soporte_Abierto_Limpio', 'Bodega_Origen']
        columnas_presentes = [col for col in columnas_necesarias if col in joined_df.columns]
        columnas_faltantes = [col for col in columnas_necesarias if col not in joined_df.columns]

        if columnas_faltantes:
            st.error(f"üö´ Columnas faltantes: {', '.join(columnas_faltantes)}")

        if len(columnas_presentes) == len(columnas_necesarias):
            riesgo_df = joined_df[
                (joined_df['Ultima_Revision'].notna()) &
                (joined_df['Ticket_Soporte_Abierto_Limpio'].notna()) &
                (joined_df['Bodega_Origen'].notna())
                ].copy()
            st.success(f"‚úÖ Datos v√°lidos encontrados: {len(riesgo_df)} registros")
        else:
            riesgo_df = pd.DataFrame()

        if len(riesgo_df) > 0:
            # Calcular d√≠as desde √∫ltima revisi√≥n si no existe
            if 'Dias_desde_revision' not in riesgo_df.columns:
                hoy = pd.Timestamp.now()
                riesgo_df['Dias_desde_revision'] = (hoy - riesgo_df['Ultima_Revision']).dt.days

            # Convertir tickets a num√©rico
            riesgo_df['Ticket_Abierto_Num'] = (riesgo_df['Ticket_Soporte_Abierto_Limpio'] == 'Si').astype(int)

            # Agregar por Bodega
            riesgo_agg = riesgo_df.groupby('Bodega_Origen').agg({
                'Dias_desde_revision': 'mean',
                'Ticket_Abierto_Num': 'mean',
                'Satisfaccion_NPS': 'mean',
                'SKU_ID': 'nunique',
                'Transaccion_ID': 'count'
            }).reset_index()
            riesgo_agg.columns = ['Bodega_Origen', 'Dias_Promedio_Sin_Revision', 'Tasa_Tickets', 'NPS_Promedio', 'SKUs_Unicos', 'Num_Transacciones']
            riesgo_agg['Tasa_Tickets_Pct'] = riesgo_agg['Tasa_Tickets'] * 100

            # Scatter plot con zonas de riesgo
            fig_riesgo = px.scatter(
                riesgo_agg,
                x='Dias_Promedio_Sin_Revision',
                y='Tasa_Tickets_Pct',
                size='Num_Transacciones',
                color='NPS_Promedio',
                hover_data=['Bodega_Origen', 'SKUs_Unicos', 'Num_Transacciones'],
                text='Bodega_Origen',
                title='Relaci√≥n entre Antig√ºedad de Revisi√≥n de Inventario y Tasa de Tickets de Soporte',
                labels={
                    'Dias_Promedio_Sin_Revision': 'D√≠as promedio desde √∫ltima revisi√≥n',
                    'Tasa_Tickets_Pct': 'Tasa de Tickets de Soporte (%)',
                    'NPS_Promedio': 'NPS Promedio',
                    'Num_Transacciones': 'Volumen de Transacciones'
                },
                color_continuous_scale='RdYlGn_r',
                size_max=30
            )

            # Umbrales
            umbral_dias = 90
            umbral_tickets = 30

            fig_riesgo.add_hline(y=umbral_tickets, line_dash="dash", line_color="red",
                                 annotation_text=f"Umbral Cr√≠tico: {umbral_tickets}% tickets",
                                 annotation_position="right")
            fig_riesgo.add_vline(x=umbral_dias, line_dash="dash", line_color="orange",
                                 annotation_text=f"Umbral: {umbral_dias} d√≠as sin revisar",
                                 annotation_position="top")

            # Zona de alto riesgo
            fig_riesgo.add_shape(
                type="rect",
                x0=umbral_dias, x1=riesgo_agg['Dias_Promedio_Sin_Revision'].max() * 1.1,
                y0=umbral_tickets, y1=riesgo_agg['Tasa_Tickets_Pct'].max() * 1.1,
                fillcolor="red", opacity=0.15, line_width=0
            )

            fig_riesgo.update_traces(textposition='top center')
            fig_riesgo.update_layout(height=500)
            st.plotly_chart(fig_riesgo, use_container_width=True)

            # Identificar bodegas cr√≠ticas
            bodegas_criticas = riesgo_agg[
                (riesgo_agg['Dias_Promedio_Sin_Revision'] > umbral_dias) &
                (riesgo_agg['Tasa_Tickets_Pct'] > umbral_tickets)
                ].sort_values('Tasa_Tickets_Pct', ascending=False)

            if len(bodegas_criticas) > 0:
                st.error(f"üö® **{len(bodegas_criticas)} bodega(s) en zona de alto riesgo operativo:**")

                for idx, row in bodegas_criticas.iterrows():
                    st.markdown(f"""
                    - **{row['Bodega_Origen']}**: 
                      - {row['Dias_Promedio_Sin_Revision']:.0f} d√≠as sin revisar inventario
                      - {row['Tasa_Tickets_Pct']:.1f}% de tasa de tickets
                      - NPS promedio: {row['NPS_Promedio']:.1f}
                      - {row['Num_Transacciones']} transacciones procesadas
                    """)

                st.markdown("""
                ### üéØ Acciones Inmediatas Requeridas:
                1. **Auditor√≠a f√≠sica urgente** del inventario en estas bodegas
                2. **Implementar calendario de revisi√≥n peri√≥dica** (m√°x. cada 60 d√≠as)
                3. **Capacitaci√≥n al personal** sobre impacto de inventario desactualizado
                4. **An√°lisis de root cause** de tickets de soporte asociados
                """)
            else:
                st.success("‚úÖ Ninguna bodega se encuentra en la zona de alto riesgo operativo.")

            # Tabla detallada
            st.subheader("üìä Detalle por Bodega")
            st.dataframe(
                riesgo_agg.sort_values('Tasa_Tickets_Pct', ascending=False),
                use_container_width=True,
                column_config={
                    "Dias_Promedio_Sin_Revision": st.column_config.NumberColumn("D√≠as sin revisi√≥n", format="%.0f"),
                    "Tasa_Tickets_Pct": st.column_config.NumberColumn("Tasa de tickets (%)", format="%.1f%%"),
                    "NPS_Promedio": st.column_config.NumberColumn("NPS Promedio", format="%.1f"),
                    "Num_Transacciones": st.column_config.NumberColumn("Transacciones", format="%d")
                }
            )

            # An√°lisis de correlaci√≥n
            corr_revision_tickets = riesgo_agg[['Dias_Promedio_Sin_Revision', 'Tasa_Tickets_Pct']].corr().iloc[0, 1]
            corr_revision_nps = riesgo_agg[['Dias_Promedio_Sin_Revision', 'NPS_Promedio']].corr().iloc[0, 1]

            corr_col1, corr_col2, corr_col3 = st.columns(3)
            corr_col1.metric(
                "Correlaci√≥n: D√≠as sin revisar vs Tickets",
                f"{corr_revision_tickets:.3f}",
                help="Correlaci√≥n positiva indica que m√°s d√≠as sin revisar = m√°s tickets"
            )
            corr_col2.metric(
                "Correlaci√≥n: D√≠as sin revisar vs NPS",
                f"{corr_revision_nps:.3f}",
                help="Correlaci√≥n negativa indica que m√°s d√≠as sin revisar = menor NPS"
            )
            corr_col3.metric(
                "Bodegas auditadas",
                len(riesgo_agg)
            )

            st.markdown(f"""
            ### üìà Interpretaci√≥n:
            - {'‚úÖ **Correlaci√≥n positiva detectada**' if corr_revision_tickets > 0.3 else '‚ö†Ô∏è Correlaci√≥n d√©bil'} 
              entre antig√ºedad de revisi√≥n y tickets de soporte ({corr_revision_tickets:.2f})
            - {'üö® **Correlaci√≥n negativa confirmada**' if corr_revision_nps < -0.2 else '‚ÑπÔ∏è Impacto limitado'} 
              entre antig√ºedad de revisi√≥n y NPS ({corr_revision_nps:.2f})
            
            **Conclusi√≥n**: {'Las bodegas que no revisan su inventario frecuentemente generan m√°s problemas operativos y menor satisfacci√≥n del cliente.' if abs(corr_revision_nps) > 0.2 else 'El impacto de la frecuencia de revisi√≥n en NPS es limitado. Considerar otros factores.'}
            """)
        else:
            st.warning("‚ö†Ô∏è No hay datos suficientes para el an√°lisis de riesgo operativo (requiere fechas de revisi√≥n y tickets).")

else:
    st.info("Por favor, carga los tres archivos CSV en el panel lateral para comenzar.")

    # Placeholder for structure info if user needs help
    with st.expander("Ver estructura esperada de archivos"):
        st.write("**Inventario:** SKU_ID, Stock_Actual, Costo_Unitario_USD, Categoria, Bodega_Origen, Lead_Time_Dias, Ultima_Revision")
        st.write("**Transacciones:** Transaccion_ID, SKU_ID, Fecha_Venta, Cantidad_Vendida, Precio_Venta_Final, Ciudad_Destino, Canal_Venta, Estado_Envio")
        st.write("**Feedback:** Feedback_ID, Transaccion_ID, Satisfaccion_NPS, Comentario_Texto, Recomienda_Marca")

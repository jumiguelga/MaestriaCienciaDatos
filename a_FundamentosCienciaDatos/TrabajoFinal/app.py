# =============================================================================
# IMPORTS & CONFIG
# =============================================================================
import io
import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px


def _try_convert_to_numeric(ser: pd.Series):
    """
    Intenta convertir una serie de strings a numÃ©rico.
    Soporta: "986,101", "8,874,909" (coma como miles); "1,234.56" (US); "1.234,56" (europeo).
    Retorna (serie_numerica, proporcion_exitosos) o (None, 0) si no convierte bien.
    """
    s = ser.astype(str).str.strip()
    empty = s.isin(["", "nan", "n/a", "none", "na"])
    non_empty = ~empty & s.notna()
    n_non_empty = non_empty.sum()
    if n_non_empty == 0:
        return None, 0.0

    def try_format(cleaner_fn):
        cleaned = s.copy()
        cleaned = cleaner_fn(cleaned)
        converted = pd.to_numeric(cleaned, errors="coerce")
        ok = converted.notna() & non_empty
        return converted, ok.sum() / n_non_empty

    # Formato 1: coma como miles (986,101 -> 986101; 1,234.56 -> 1234.56)
    conv1, rate1 = try_format(lambda x: x.str.replace(",", "", regex=False))
    # Formato 2: coma como decimal (1.234,56 -> 1234.56)
    conv2, rate2 = try_format(lambda x: x.str.replace(".", "", regex=False).str.replace(",", ".", regex=False))

    if rate1 >= rate2 and rate1 >= 0.8:
        return conv1, rate1
    if rate2 >= 0.8:
        return conv2, rate2
    return None, 0.0


def donut_issue_chart(count_issue: int, total: int, titulo: str, etiqueta_issue: str, etiqueta_ok: str) -> None:
    """
    Muestra un grÃ¡fico tipo donut (pie chart con agujero) para resumir
    la proporciÃ³n de filas con problema vs filas sin problema.
    """
    if total <= 0:
        return

    count_issue = int(count_issue)
    total = int(total)
    count_ok = max(total - count_issue, 0)

    data = pd.DataFrame(
        {
            "estado": [etiqueta_issue, etiqueta_ok],
            "filas": [count_issue, count_ok],
        }
    )

    fig = px.pie(
        data,
        names="estado",
        values="filas",
        hole=0.6,
        color="estado",
        color_discrete_map={
            etiqueta_issue: "#e74c3c",  # rojo para filas con problema
            etiqueta_ok: "#2ecc71",     # verde para filas sin problema
        },
    )
    fig.update_traces(textinfo="percent", textfont_size=14)
    fig.update_layout(
        title=titulo,
        showlegend=True,
        margin=dict(l=0, r=0, t=40, b=0),
    )

    st.plotly_chart(fig, use_container_width=True)


# ConfiguraciÃ³n de la pÃ¡gina
st.set_page_config(
    page_title="Panel inteligente",
    page_icon="ðŸ¤–",
    layout="wide"
)

# =============================================================================
# SIDEBAR: DATA LOADING
# =============================================================================

# Sidebar for data input
st.sidebar.title("ðŸ“ Carga de Datos")

# Option selector
data_source = st.sidebar.radio(
    "Selecciona el tipo de archivo:",
    ["Archivo CSV", "Archivo JSON", "URL"]
)

df = None

# Handle CSV file upload
if data_source == "Archivo CSV":
    uploaded_file = st.sidebar.file_uploader(
        "Cargar archivo CSV",
        type=['csv'],
        help="Cargar un archivo CSV para analizar"
    )
    if uploaded_file is not None:
        try:
            df = pd.read_csv(uploaded_file)
            st.sidebar.success("Archivo CSV cargado exitosamente!")
        except Exception as e:
            st.sidebar.error(f"Error al cargar el archivo CSV: {str(e)}")

# Handle JSON file upload
elif data_source == "Archivo JSON":
    uploaded_file = st.sidebar.file_uploader(
        "Cargar archivo JSON",
        type=['json'],
        help="Cargar un archivo JSON para analizar"
    )
    if uploaded_file is not None:
        try:
            df = pd.read_json(uploaded_file)
            st.sidebar.success("Archivo JSON cargado exitosamente!")
        except Exception as e:
            st.sidebar.error(f"Error al cargar el archivo JSON: {str(e)}")

# Handle URL input
elif data_source == "URL":
    url = st.sidebar.text_input(
        "Ingresar URL",
        placeholder="https://example.com/data.csv",
        help="Ingresar URL a un archivo CSV o JSON"
    )
    if url:
        try:
            if url.endswith('.json'):
                df = pd.read_json(url)
            else:
                df = pd.read_csv(url)
            st.sidebar.success("Datos cargados desde la URL exitosamente!")
        except Exception as e:
            st.sidebar.error(f"Error al cargar desde la URL: {str(e)}")

# =============================================================================
# SIDEBAR: GLOBAL FILTERS / DATA TREATMENT
# =============================================================================

delete_duplicates = False
imputation_method = "Sin imputaciÃ³n"
treat_outliers = False

if df is not None:
    st.sidebar.markdown("---")
    st.sidebar.subheader("âš™ï¸ Filtros y tratamiento de datos")

    delete_duplicates = st.sidebar.checkbox(
        "Eliminar filas duplicadas",
        value=False,
        help="Si se marca, se eliminarÃ¡n las filas detectadas como duplicadas.",
    )

    imputation_method = st.sidebar.selectbox(
        "MÃ©todo de imputaciÃ³n numÃ©rica",
        options=["Sin imputaciÃ³n", "Media", "Mediana", "Cero"],
        help="CÃ³mo imputar valores numÃ©ricos faltantes (NaN).",
    )

    treat_outliers = st.sidebar.checkbox(
        "Eliminar filas con valores atÃ­picos",
        value=False,
        help="Si se marca, se eliminarÃ¡n las filas marcadas con valores atÃ­picos.",
    )

    st.sidebar.markdown("---")
    st.sidebar.subheader("ðŸ”‘ Columna Ã­ndice / identificador")
    index_column = st.sidebar.selectbox(
        "Selecciona la columna identificadora (no se usarÃ¡ para anÃ¡lisis estadÃ­stico).",
        options=df.columns.tolist(),
        index=0,
    )
    st.session_state["index_column"] = index_column

# =============================================================================
# MAIN: DISPLAY (df.info, df.head)
# =============================================================================

# Ãrea principal
st.title("ðŸ“Š Panel inteligente")

if df is not None:
    tab_ingesta, tab_visualizacion, tab_insights_ia = st.tabs(["Ingesta y Procesamiento de Datos (ETL)", "VisualizaciÃ³n DinÃ¡mica (EDA)", "Insights de IA"])

    with tab_ingesta:
        # 1) Reemplazar vacÃ­os por NaN
        df_proc = df.replace("", np.nan).replace(r"^\s*$", np.nan, regex=True)
        # Incluir object, string y category
        object_cols = [
            c for c in df_proc.columns
            if (
                df_proc[c].dtype == object
                or df_proc[c].dtype.name == "object"
                or pd.api.types.is_string_dtype(df_proc[c])
                or str(df_proc[c].dtype) == "category"
            )
        ]
        # 2a) Intentar convertir columnas objeto que parecen numÃ©ricas (ej. "986,101", "8,874,909")
        cols_converted_to_numeric = []
        for col in object_cols:
            s = df_proc[col]
            if s.dtype.name == "category":
                s = s.astype(object)
            num_ser, rate = _try_convert_to_numeric(s)
            if num_ser is not None:
                df_proc[col] = num_ser
                cols_converted_to_numeric.append(col)
        object_cols = [c for c in object_cols if c not in cols_converted_to_numeric]
        # 2b) Sanitizar el resto: trim y minÃºsculas (unifica "Fiber optic" y " Fiber optic ")
        for col in object_cols:
            s = df_proc[col]
            s = s.astype(str).str.strip().str.lower()
            s = s.replace("nan", np.nan)
            df_proc[col] = s

        st.subheader("1. Reemplazo de valores vacÃ­os por NaN")
        st.caption("Se convirtieron cadenas vacÃ­as y espacios en blanco a NaN.")
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("Filas", df_proc.shape[0])
        with col2:
            st.metric("Columnas", df_proc.shape[1])
        with col3:
            st.metric("Valores NaN tras reemplazo", int(df_proc.isna().sum().sum()))

        st.subheader("2. SanitizaciÃ³n y conversiÃ³n numÃ©rica")
        st.caption(
            "Se detectaron strings con formato numÃ©rico (ej. 986,101, 8,874,909) y se convirtieron a numÃ©rico. "
            "El resto de columnas texto: trim y minÃºsculas."
        )
        col_s1, col_s2 = st.columns(2)
        with col_s1:
            st.metric("Columnas convertidas a numÃ©rico (formato localizado)", len(cols_converted_to_numeric))
        with col_s2:
            st.metric("Columnas categÃ³ricas sanitizadas", len(object_cols))
        if cols_converted_to_numeric:
            st.caption(f"Convertidas a numÃ©rico: {', '.join(cols_converted_to_numeric)}.")
        if object_cols:
            st.caption(f"Columnas categÃ³ricas: {', '.join(object_cols)}.")

        # Resumen del dataset (sobre df_proc ya sanitizado). Identificar booleanas por valores Ãºnicos (genÃ©rico).
        st.subheader("InformaciÃ³n del dataset")
        n_rows, n_cols = len(df_proc), len(df_proc.columns)
        numeric_cols = set(df_proc.select_dtypes(include=[np.number]).columns)
        bool_cols = set(df_proc.select_dtypes(include=[bool]).columns)
        other_cols = [c for c in df_proc.columns if c not in numeric_cols and c not in bool_cols]
        # Valores a ignorar al contar Ãºnicos (no cuentan como "valor real")
        skip_vals = {"", "nan", "n/a", "none", "na"}
        def get_effective_unique_set(ser):
            """Conjunto de valores Ãºnicos normalizados (strip, lower), sin skip_vals. Para numÃ©ricos devuelve set de enteros."""
            if ser.dtype.kind in "iufb":
                return set(pd.Series(ser.dropna().astype(int).unique()).astype(str))
            s = ser.dropna().astype(str).str.strip().str.lower()
            s = s[~s.isin(skip_vals)]
            return set(s.unique())
        # Booleana = exactamente 2 valores Ãºnicos efectivos (yes/no, sÃ­/no, 0/1, etc.) â€” sin nombres quemados
        infer_bool_cols = [c for c in other_cols if len(get_effective_unique_set(df_proc[c])) == 2]
        numeric_bool_cols = [c for c in numeric_cols if get_effective_unique_set(df_proc[c]) == {"0", "1"}]
        categorical_cols = [c for c in other_cols if c not in infer_bool_cols]
        n_numeric = len(numeric_cols)
        n_bool = len(bool_cols) + len(infer_bool_cols) + len(numeric_bool_cols)
        n_categorical = len(categorical_cols)

        c1, c2, c3, c4, c5 = st.columns(5)
        with c1:
            st.metric("Total columnas", n_cols)
        with c2:
            st.metric("Total filas", n_rows)
        with c3:
            st.metric("Columnas categÃ³ricas", n_categorical)
        with c4:
            st.metric("Columnas numÃ©ricas", n_numeric)
        with c5:
            st.metric("Columnas booleanas", n_bool)
        if infer_bool_cols or numeric_bool_cols:
            parts = []
            if infer_bool_cols:
                parts.append(f"Inferidas (yes/no, sÃ­/no, etc.): {', '.join(infer_bool_cols)}.")
            if numeric_bool_cols:
                parts.append(f"NumÃ©ricas 0/1: {', '.join(numeric_bool_cols)}.")
            st.caption("Se consideran booleanas las columnas con solo dos valores. " + " ".join(parts))

        # Tabla: convertir columnas inferidas como booleanas a 1 y 0 (orden estable: sorted -> 0, 1)
        df_display = df_proc.copy()
        for col in infer_bool_cols:
            unicos = sorted(df_display[col].dropna().unique().tolist(), key=str)
            if len(unicos) == 2:
                df_display[col] = df_display[col].map({unicos[0]: 0, unicos[1]: 1})
        st.subheader("Primeras 5 filas (tras sanitizaciÃ³n y conversiÃ³n a 1/0 en booleanas)")
        st.dataframe(df_display.head(), use_container_width=True)

        # Debug: mismos valores que la tabla anterior (sanitizados + booleanas como 0/1)
        with st.expander("ðŸ” Debug: valores Ãºnicos por columna (valor â†’ conteo)"):
            st.caption("Valores actualizados: sanitizaciÃ³n aplicada y columnas booleanas mostradas como 0 y 1.")
            for col in df_display.columns:
                st.markdown(f"**{col}**")
                counts = df_display[col].value_counts(dropna=False)
                st.dataframe(counts.rename_axis("valor").reset_index(name="conteo"), use_container_width=True, hide_index=True)
                st.divider()

        # Columna Nulos: True si alguna columna es nula en esa fila
        df_proc["Nulos"] = df_proc.drop(columns=["Nulos"], errors="ignore").isna().any(axis=1)
        n_filas_nulas = df_proc["Nulos"].sum()

        st.subheader("3. DetecciÃ³n de nulos por fila")
        st.caption('Columna "Nulos": True si la fila tiene al menos un valor nulo.')
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("Filas con al menos un nulo", int(n_filas_nulas))
        with col2:
            st.metric("Filas sin nulos", int((~df_proc["Nulos"]).sum()))
        with col3:
            pct = 100 * n_filas_nulas / len(df_proc) if len(df_proc) else 0
            st.metric("% filas con nulos", f"{pct:.1f}%")
        
        # Columna Duplicados
        df_proc["Duplicados"] = df_proc.drop(columns=["Nulos", "Duplicados"], errors="ignore").duplicated(keep=False)
        n_dup = df_proc["Duplicados"].sum()

        st.subheader("4. DetecciÃ³n de duplicados por fila")
        st.caption('Columna "Duplicados": True si la fila estÃ¡ duplicada (considerando todas las columnas originales).')
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("Filas duplicadas", int(n_dup))
        with col2:
            st.metric("Filas Ãºnicas", int((~df_proc["Duplicados"]).sum()))
        with col3:
            pct_dup = 100 * n_dup / len(df_proc) if len(df_proc) else 0
            st.metric("% filas duplicadas", f"{pct_dup:.1f}%")
        
        # Valores atÃ­picos (solo columnas numÃ©ricas, mÃ©todo IQR)
        numericas = [c for c in df_proc.select_dtypes(include=[np.number]).columns if c not in ("Nulos", "Duplicados", "Valores AtÃ­picos")]
        atipico_fila = pd.Series(False, index=df_proc.index)
        if numericas:
            for col in numericas:
                Q1 = df_proc[col].quantile(0.25)
                Q3 = df_proc[col].quantile(0.75)
                IQR = Q3 - Q1
                if IQR > 0:
                    fuera = (df_proc[col] < Q1 - 1.5 * IQR) | (df_proc[col] > Q3 + 1.5 * IQR)
                    atipico_fila = atipico_fila | fuera
        df_proc["Valores AtÃ­picos"] = atipico_fila
        n_atipicos = df_proc["Valores AtÃ­picos"].sum()

        st.subheader("5. DetecciÃ³n de valores atÃ­picos por fila")
        st.caption('Columna "Valores AtÃ­picos": True si en esa fila algÃºn valor numÃ©rico es atÃ­pico (mÃ©todo IQR).')
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("Filas con al menos un atÃ­pico", int(n_atipicos))
        with col2:
            st.metric("Filas sin atÃ­picos", int((~df_proc["Valores AtÃ­picos"]).sum()))
        with col3:
            pct_at = 100 * n_atipicos / len(df_proc) if len(df_proc) else 0
            st.metric("% filas con atÃ­picos", f"{pct_at:.1f}%")

        # GrÃ¡ficos resumen por tipo de problema en una sola fila
        c_nulos, c_dup, c_atip = st.columns(3)
        with c_nulos:
            donut_issue_chart(
                count_issue=n_filas_nulas,
                total=len(df_proc),
                titulo="DistribuciÃ³n de filas con nulos",
                etiqueta_issue="Con nulos",
                etiqueta_ok="Sin nulos",
            )
        with c_dup:
            donut_issue_chart(
                count_issue=n_dup,
                total=len(df_proc),
                titulo="DistribuciÃ³n de filas duplicadas",
                etiqueta_issue="Duplicadas",
                etiqueta_ok="Ãšnicas",
            )
        with c_atip:
            donut_issue_chart(
                count_issue=n_atipicos,
                total=len(df_proc),
                titulo="DistribuciÃ³n de filas con valores atÃ­picos",
                etiqueta_issue="Con atÃ­picos",
                etiqueta_ok="Sin atÃ­picos",
            )

        # Aplicar filtros globales configurados en la barra lateral
        df_filtrado = df_proc.copy()

        # ImputaciÃ³n numÃ©rica
        valores_imputados = 0
        if imputation_method != "Sin imputaciÃ³n":
            columnas_numericas_tratadas = [
                c
                for c in df_filtrado.select_dtypes(include=[np.number]).columns
                if c not in ("Nulos", "Duplicados", "Valores AtÃ­picos")
            ]
            for col in columnas_numericas_tratadas:
                n_nan_antes = df_filtrado[col].isna().sum()
                if n_nan_antes == 0:
                    continue
                if imputation_method == "Media":
                    valor = df_filtrado[col].mean()
                elif imputation_method == "Mediana":
                    valor = df_filtrado[col].median()
                else:  # "Cero"
                    valor = 0
                df_filtrado[col] = df_filtrado[col].fillna(valor)
                valores_imputados += n_nan_antes

        filas_antes_filtros = len(df_filtrado)

        # EliminaciÃ³n de duplicados
        filas_eliminadas_duplicados = 0
        if delete_duplicates:
            if "Duplicados" in df_filtrado.columns:
                filas_eliminadas_duplicados = int(df_filtrado["Duplicados"].sum())
                df_filtrado = df_filtrado[~df_filtrado["Duplicados"]]

        # EliminaciÃ³n de filas con valores atÃ­picos
        filas_eliminadas_atipicos = 0
        if treat_outliers:
            if "Valores AtÃ­picos" in df_filtrado.columns:
                filas_eliminadas_atipicos = int(df_filtrado["Valores AtÃ­picos"].sum())
                df_filtrado = df_filtrado[~df_filtrado["Valores AtÃ­picos"]]

        filas_despues_filtros = len(df_filtrado)

        # Guardar dataset filtrado en sesiÃ³n para futuras pestaÃ±as
        st.session_state["df_filtrado"] = df_filtrado

        # Column metadata para EDA: excluir index_column y columnas auxiliares
        exclude_cols = {"Nulos", "Duplicados", "Valores AtÃ­picos"}
        if "index_column" in st.session_state:
            exclude_cols.add(st.session_state["index_column"])

        # Numeric: puras (excluir 0/1 binarias que son booleanas)
        numeric_cols_for_eda = [
            c for c in df_filtrado.select_dtypes(include=[np.number]).columns
            if c not in exclude_cols and c not in numeric_bool_cols
        ]
        # Categorical
        categorical_cols_for_eda = [c for c in categorical_cols if c not in exclude_cols]
        # Boolean: inferidas + numÃ©ricas 0/1 + nativas
        all_bool_cols = list(infer_bool_cols) + list(numeric_bool_cols) + list(bool_cols)
        boolean_cols_for_eda = [c for c in all_bool_cols if c not in exclude_cols]
        # Datetime: detectar columnas de fecha nativas
        datetime_cols_for_eda = [
            c for c in df_filtrado.columns
            if c not in exclude_cols and pd.api.types.is_datetime64_any_dtype(df_filtrado[c])
        ]

        st.session_state["column_metadata"] = {
            "numeric": numeric_cols_for_eda,
            "categorical": categorical_cols_for_eda,
            "boolean": boolean_cols_for_eda,
            "datetime": datetime_cols_for_eda,
        }

        st.divider()
        st.subheader("Resumen del procesamiento")
        st.success(
            f"Dataset procesado: {df_proc.shape[0]} filas originales, {df_filtrado.shape[0]} filas tras filtros, "
            f"{df_proc.shape[1]} columnas. "
            "Se aplicÃ³: reemplazo de vacÃ­os por NaN; sanitizaciÃ³n de categÃ³ricas (trim de espacios y conversiÃ³n a minÃºsculas); "
            "detecciÃ³n de nulos, duplicados y valores atÃ­picos; columnas con solo dos valores (ej. sÃ­/no) tratadas como booleanas y mostradas como 1/0. "
            "Revisa las secciones anteriores para los detalles."
        )

        st.markdown("**Filtros de tratamiento seleccionados**")
        if (
            imputation_method == "Sin imputaciÃ³n"
            and not delete_duplicates
            and not treat_outliers
        ):
            st.caption("No se aplicÃ³ ningÃºn filtro adicional sobre el dataset.")
        else:
            if imputation_method != "Sin imputaciÃ³n":
                st.markdown(
                    f"- ImputaciÃ³n numÃ©rica: **{imputation_method}** "
                    f"(valores imputados: {valores_imputados})."
                )
            if delete_duplicates:
                st.markdown(
                    f"- EliminaciÃ³n de duplicados: **{filas_eliminadas_duplicados}** filas marcadas como duplicadas."
                )
            if treat_outliers:
                st.markdown(
                    f"- EliminaciÃ³n de valores atÃ­picos: **{filas_eliminadas_atipicos}** filas marcadas con valores atÃ­picos."
                )
            if filas_antes_filtros != filas_despues_filtros:
                st.markdown(
                    f"- Filas totales antes de filtros: **{filas_antes_filtros}**; "
                    f"despuÃ©s de filtros: **{filas_despues_filtros}**."
                )

    with tab_visualizacion:
        if "df_filtrado" not in st.session_state:
            st.info("Ejecuta primero el procesamiento en la pestaÃ±a **Ingesta y Procesamiento de Datos (ETL)** para habilitar la visualizaciÃ³n.")
        else:
            df_viz = st.session_state["df_filtrado"].copy()
            meta = st.session_state.get("column_metadata", {})
            numeric_cols = meta.get("numeric", [])
            categorical_cols = meta.get("categorical", [])
            boolean_cols = meta.get("boolean", [])
            datetime_cols = meta.get("datetime", [])

            # ========== FILTROS GLOBALES ==========
            st.subheader("Filtros globales")
            with st.expander("Configurar filtros", expanded=False):
                # Rango de fechas
                if datetime_cols:
                    date_col = st.selectbox("Columna de fecha para filtrar", datetime_cols, key="eda_date_col")
                    if date_col and date_col in df_viz.columns:
                        col_ser = pd.to_datetime(df_viz[date_col], errors="coerce").dropna()
                        if len(col_ser) > 0:
                            min_d = col_ser.min()
                            max_d = col_ser.max()
                            min_date = min_d.date() if hasattr(min_d, "date") else min_d
                            max_date = max_d.date() if hasattr(max_d, "date") else max_d
                            d1, d2 = st.columns(2)
                            with d1:
                                date_start = st.date_input("Desde", value=min_date, min_value=min_date, max_value=max_date, key="eda_date_start")
                            with d2:
                                date_end = st.date_input("Hasta", value=max_date, min_value=min_date, max_value=max_date, key="eda_date_end")
                            if date_start and date_end and date_start <= date_end:
                                df_viz[date_col] = pd.to_datetime(df_viz[date_col], errors="coerce")
                                df_viz = df_viz[(df_viz[date_col].dt.date >= date_start) & (df_viz[date_col].dt.date <= date_end)]
                else:
                    st.caption("No se encontraron columnas de fecha para filtrar.")

                # CategorÃ­as
                if categorical_cols:
                    cat_filter_col = st.selectbox("Columna categÃ³rica a filtrar", ["(Ninguna)"] + categorical_cols, key="eda_cat_col")
                    if cat_filter_col != "(Ninguna)" and cat_filter_col in df_viz.columns:
                        vals = df_viz[cat_filter_col].dropna().unique().tolist()
                        selected_cats = st.multiselect("Valores a incluir (vacÃ­o = todos)", vals, default=vals, key="eda_cat_vals")
                        if selected_cats:
                            df_viz = df_viz[df_viz[cat_filter_col].isin(selected_cats)]

                # Sliders numÃ©ricos
                if numeric_cols:
                    num_filter_col = st.selectbox("Columna numÃ©rica a filtrar", ["(Ninguna)"] + numeric_cols, key="eda_num_col")
                    if num_filter_col != "(Ninguna)" and num_filter_col in df_viz.columns:
                        ser = df_viz[num_filter_col].dropna()
                        if len(ser) > 0:
                            lo, hi = float(ser.min()), float(ser.max())
                            rng = st.slider("Rango", lo, hi, (lo, hi), key="eda_num_range")
                            df_viz = df_viz[(df_viz[num_filter_col] >= rng[0]) & (df_viz[num_filter_col] <= rng[1])]

            if len(df_viz) == 0:
                st.warning("No hay datos tras aplicar los filtros. Ajusta los filtros o vuelve a cargar el dataset.")
            else:
                # ========== SUB-TABS: Univariado, Bivariado, Reporte ==========
                tab_uni, tab_bi, tab_rep = st.tabs(["AnÃ¡lisis Univariado", "AnÃ¡lisis Bivariado", "Reporte"])

                with tab_uni:
                    st.subheader("AnÃ¡lisis univariado")
                    # Numeric
                    if numeric_cols:
                        with st.expander("Columnas numÃ©ricas", expanded=True):
                            num_sel = st.selectbox("Seleccionar columna numÃ©rica", numeric_cols, key="uni_num")
                            nbins = st.slider("Bins del histograma", 5, 150, 30, key="uni_nbins")
                            c1, c2 = st.columns(2)
                            with c1:
                                fig_hist = px.histogram(df_viz, x=num_sel, nbins=nbins, title=f"Histograma: {num_sel}")
                                fig_hist.update_layout(margin=dict(l=0, r=0, t=40, b=0))
                                st.plotly_chart(fig_hist, use_container_width=True)
                            with c2:
                                fig_box = px.box(df_viz, y=num_sel, title=f"Boxplot: {num_sel}")
                                fig_box.update_layout(margin=dict(l=0, r=0, t=40, b=0))
                                st.plotly_chart(fig_box, use_container_width=True)
                    # Categorical
                    if categorical_cols:
                        with st.expander("Columnas categÃ³ricas", expanded=True):
                            cat_sel = st.selectbox("Seleccionar columna categÃ³rica", categorical_cols, key="uni_cat")
                            show_pct = st.checkbox("Mostrar porcentajes", value=False, key="uni_pct")
                            counts = df_viz[cat_sel].value_counts(dropna=False).reset_index()
                            counts.columns = ["categoria", "conteo"]
                            if show_pct:
                                total = counts["conteo"].sum()
                                counts["porcentaje"] = (counts["conteo"] / total * 100).round(1)
                                y_col = "porcentaje"
                            else:
                                y_col = "conteo"
                            fig_bar = px.bar(counts, x="categoria", y=y_col, title=f"DistribuciÃ³n: {cat_sel}")
                            fig_bar.update_layout(margin=dict(l=0, r=0, t=40, b=0), xaxis_tickangle=-45)
                            st.plotly_chart(fig_bar, use_container_width=True)
                    # Boolean
                    if boolean_cols:
                        with st.expander("Columnas booleanas", expanded=True):
                            bool_sel = st.selectbox("Seleccionar columna booleana", boolean_cols, key="uni_bool")
                            vc = df_viz[bool_sel].value_counts(dropna=False).reset_index()
                            vc.columns = ["valor", "conteo"]
                            fig_bool = px.bar(vc, x="valor", y="conteo", title=f"DistribuciÃ³n: {bool_sel}")
                            fig_bool.update_layout(margin=dict(l=0, r=0, t=40, b=0))
                            st.plotly_chart(fig_bool, use_container_width=True)
                    if not numeric_cols and not categorical_cols and not boolean_cols:
                        st.caption("No hay columnas disponibles para anÃ¡lisis univariado.")

                with tab_bi:
                    st.subheader("AnÃ¡lisis bivariado")
                    # Correlaciones
                    if len(numeric_cols) >= 2:
                        with st.expander("Correlaciones (heatmap)", expanded=True):
                            corr_df = df_viz[numeric_cols].corr()
                            fig_corr = px.imshow(
                                corr_df,
                                text_auto=".2f",
                                color_continuous_scale="RdBu_r",
                                aspect="auto",
                                title="Matriz de correlaciÃ³n",
                            )
                            fig_corr.update_layout(margin=dict(l=0, r=0, t=40, b=0), height=400)
                            st.plotly_chart(fig_corr, use_container_width=True)
                    else:
                        st.caption("Se requieren al menos 2 columnas numÃ©ricas para el heatmap de correlaciones.")
                    # EvoluciÃ³n temporal
                    if datetime_cols:
                        with st.expander("EvoluciÃ³n temporal", expanded=True):
                            dt_col = st.selectbox("Columna de fecha (eje X)", datetime_cols, key="bi_dt")
                            agg_col = st.selectbox("Columna a agregar (eje Y)", ["(Conteo)"] + numeric_cols, key="bi_agg")
                            agg_func = st.radio("FunciÃ³n de agregaciÃ³n", ["mean", "sum", "count"], horizontal=True, key="bi_agg_func")
                            df_temp = df_viz.copy()
                            df_temp[dt_col] = pd.to_datetime(df_temp[dt_col], errors="coerce")
                            df_temp = df_temp.dropna(subset=[dt_col])
                            if agg_col == "(Conteo)":
                                ts = df_temp.groupby(df_temp[dt_col].dt.to_period("D").dt.to_timestamp()).size().reset_index(name="conteo")
                                ts.columns = [dt_col, "conteo"]
                                fig_ts = px.area(ts, x=dt_col, y="conteo", title="EvoluciÃ³n temporal (conteo)")
                            else:
                                if agg_func == "count":
                                    ts = df_temp.groupby(df_temp[dt_col].dt.to_period("D").dt.to_timestamp())[agg_col].count().reset_index()
                                elif agg_func == "sum":
                                    ts = df_temp.groupby(df_temp[dt_col].dt.to_period("D").dt.to_timestamp())[agg_col].sum().reset_index()
                                else:
                                    ts = df_temp.groupby(df_temp[dt_col].dt.to_period("D").dt.to_timestamp())[agg_col].mean().reset_index()
                                fig_ts = px.line(ts, x=dt_col, y=agg_col, title=f"EvoluciÃ³n temporal ({agg_col} - {agg_func})")
                            fig_ts.update_layout(margin=dict(l=0, r=0, t=40, b=0))
                            st.plotly_chart(fig_ts, use_container_width=True)
                    else:
                        st.caption("No se encontraron columnas de fecha para evoluciÃ³n temporal.")
                    # Cross-tabs opcionales
                    if numeric_cols and categorical_cols:
                        with st.expander("NumÃ©rico vs categÃ³rico (boxplot por categorÃ­a)", expanded=False):
                            bi_num = st.selectbox("Columna numÃ©rica (Y)", numeric_cols, key="bi_num")
                            bi_cat = st.selectbox("Columna categÃ³rica (X)", categorical_cols, key="bi_cat")
                            fig_cross = px.box(df_viz, x=bi_cat, y=bi_num, title=f"{bi_num} por {bi_cat}")
                            fig_cross.update_layout(margin=dict(l=0, r=0, t=40, b=0), xaxis_tickangle=-45)
                            st.plotly_chart(fig_cross, use_container_width=True)
                    if len(numeric_cols) >= 2:
                        with st.expander("NumÃ©rico vs numÃ©rico (scatter)", expanded=False):
                            sc_x = st.selectbox("Eje X", numeric_cols, key="sc_x")
                            sc_y_opts = [c for c in numeric_cols if c != sc_x] or numeric_cols
                            sc_y = st.selectbox("Eje Y", sc_y_opts, key="sc_y")
                            if sc_x and sc_y:
                                fig_sc = px.scatter(df_viz, x=sc_x, y=sc_y, title=f"{sc_x} vs {sc_y}")
                                fig_sc.update_layout(margin=dict(l=0, r=0, t=40, b=0))
                                st.plotly_chart(fig_sc, use_container_width=True)

                with tab_rep:
                    st.subheader("Reporte")
                    col1, col2, col3, col4 = st.columns(4)
                    with col1:
                        st.metric("Filas", len(df_viz))
                    with col2:
                        st.metric("Columnas", len(df_viz.columns))
                    with col3:
                        pct_miss = 100 * df_viz.isna().sum().sum() / (len(df_viz) * len(df_viz.columns)) if len(df_viz) > 0 else 0
                        st.metric("% valores faltantes", f"{pct_miss:.1f}%")
                    with col4:
                        st.metric("Columnas numÃ©ricas", len(numeric_cols))
                    st.subheader("Resumen estadÃ­stico (columnas numÃ©ricas)")
                    if numeric_cols:
                        st.dataframe(df_viz[numeric_cols].describe(), use_container_width=True)
                    else:
                        st.caption("No hay columnas numÃ©ricas.")
                    st.subheader("Moda por columna categÃ³rica/booleana")
                    cat_bool = categorical_cols + boolean_cols
                    if cat_bool:
                        modas = {c: df_viz[c].mode().iloc[0] if len(df_viz[c].mode()) > 0 else "â€”" for c in cat_bool}
                        st.dataframe(pd.DataFrame({"columna": list(modas.keys()), "moda": list(modas.values())}), use_container_width=True, hide_index=True)
                    csv = df_viz.to_csv(index=False).encode("utf-8")
                    st.download_button("Descargar dataset filtrado (CSV)", csv, "dataset_filtrado.csv", "text/csv", key="rep_dl")

else:
    st.info("ðŸ‘ˆ Selecciona una fuente de datos en la barra lateral y carga tu dataset para comenzar.")